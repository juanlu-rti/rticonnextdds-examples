# Recording Service -- Integration with LevelDB

## Example Description

This example is an implementation of the pluggable *RTI Recording Service* 
storage, using the no-SQL, persistent key-value store, LevelDB. This engine was
developed by Google as a fast key-value storage library that maps from key 
strings to string values. The LevelDB code can be found on [GitHub](https://github.com/google/leveldb).

This example is written in modern C++ (C++11) and shows how integration with a
no-SQL storage technology can be done in *RTI Recording Service*. The example
implements both the Storage Writer and Storage Reader plug-ins, so it can be
used to record, replay and/or convert the data coming from the Connext DDS data
bus. However, take into account that LevelDB is not meant to be accessed by two
different processes at the same time, and hence recording and replaying (or
converting) concurrently is _not supported_. In general, concurrently accessing
the LevelDB database being recorded, replayed or converted is not supported in 
this example. Note that this is not the case with our built-in SQLite 
implementation, although concurrent access in that case is also discouraged.

This example will store any DDS Topic discovered by *RTI Recording Service* that
has been marked as to be recorded (by defining the Topics and Topic Groups in 
the XML configuration).

The example provides the following files:

- `LevelDbWriter.hpp` and `LevelDbWriter.cxx` implement the Storage Writer
  API in *RTI Recording Service* and hence provides the ability for *Recorder*
  to store samples to disk, or to *Converter* to convert data _to_ LevelDB storage
  (coming from a different format or storage).
- `LevelDbReader.hpp` and `LevelDbReader.cxx` implement the Storage Reader 
  API in *RTI Recording Service* and hence provides the ability for *Replay* to
  read samples from a LevelDB database and publish them to DDS, or to *Converter*
  to convert data _from_ LevelDB storage to a different format or storage).
- `Tests.cxx` implements a test application that can be built and run on your
  system to verify the correct working of the provided Storage Writer and 
  Storage Reader classes.
- `LevelDb_RecorderTypes.idl` provides the definitions of the types that will be
  stored into the database keys and values. The code related to these types will
  be automatically generated by the build system using *RTI Code Generator*.
- `Utils.hpp` and `Utils.cxx` implement some helper functions and, more
  importantly, the `UserKeyComparator` class that is used to customise the
  sample ordering in LevelDB. Because *Recorder* and *Replay* work based on time
  (they can be though about as time series storage) we need to make sure that
  samples are ordered in reception timestamp order, ascending and monotonic.
- `leveldb_recorder.xml`, `leveldb_replay.xml` and `leveldb_converter.xml`
  provide XML configurations for *Recorder*, *Replay* and *Converter*,
  respectively. The converter configuration file contains two configurations,
  one to convert to built-in SQLite CDR (serialized) format and another one to
  convert to built-in SQLite JSON format. All configurations are set up to 
  record, replay or convert all topics discovered by the application.

For more details on how to implement custom plugins, please refer to the *RTI
Recording Service API* documentation.

## Getting LevelDB and building it

The LevelDB database engine can be obtained from GitHub (this [link](https://github.com/google/leveldb). The repository can be cloned and there are 
instructions in the documentation to build the library with `CMake`.

> **Note**:
> Note: for this example, *RTI Recording Service* requires the LevelDB library
> to be linked _dynamically_. Make sure that you specify the `BUILD_SHARED_LIBS`
> when calling `CMake` to build LevelDB.

## Building the example

This example is designed to be built with `CMake`. In order to build it, you 
need to provide the following variables to `CMake` when creating the build
system:

- `CONNEXTDDS_DIR`: it should point to the installation of *RTI Connext DDS* 
  to be used for the build.
- `CONNEXTDDS_ARCH`: the *RTI Connext DDS* Target architecture to be used in
  your system.
- `CMAKE_BUILD_TYPE`: specifies the build mode. Valid values are Release
  and Debug. The general recommendation is to build in release mode. Mixing a
  release version *RTI Recording Service* executable with debug libraries may
  have unintended consequences when running the application.
- `LEVELDB_DIR`: it should point to where the LevelDB *dynamic* library is
  located.
- `LEVELDB_INCLUDE`: it should point to where the LevelDB API header files are
  located. This will normally be the `include` directory inside the directory
  where you cloned or extracted the LevelDB source code.
- `BUILD_TESTER_APP`: optional flag used to enable/disable the building of the
  provided tester application. If not provided, the application won't be built.

Build the example code by running the following command:

```bash
mkdir build
cd build
cmake -DCONNEXTDDS_DIR=<connext directory> 
      -DCONNEXTDDS_ARCH=<connext architecture>
      -DCMAKE_BUILD_TYPE=Release 
      -DLEVELDB_DIR=<leveldb directory>
      -DLEVELDB_INCLUDE=<leveldb include directory>
      ..
cmake --build . --config Release
```

> **Note**:
>
> When using a multi-configuration generator, make sure you specify
> the `--config` parameter in your call to `cmake --build .`. In general,
> it's a good practice to always provide it.

In case you are using Windows x64, you have to add the option -A in the cmake
command as follows:

```bash
cmake -DCONNEXTDDS_DIR=<connext dir> 
      -DCONNEXTDDS_ARCH=<connext architecture>
      -DCMAKE_BUILD_TYPE=Release 
      -DBUILD_SHARED_LIBS=ON .. 
      -A x64
```

**Cross-compilation**.

When you need to cross-compile the example, the above command will not work, the
assigned compiler won't be the cross-compiler and errors may happen when linking
against the cross-compiled Connext binaries. To fix this, you have to create a
file with the architecture name and call CMake with a specific flag called
``-DCMAKE_TOOLCHAIN_FILE``. An example of the file to create with the toolchain
settings (e.g. for an ARM architecture):

```cmake
set(CMAKE_SYSTEM_NAME Linux)
set(toolchain_path "<path to>/arm-bcm2708/gcc-linaro-arm-linux-gnueabihf-raspbian")
set(CMAKE_C_COMPILER "${toolchain_path}/bin/arm-linux-gnueabihf-gcc")
set(CMAKE_CXX_COMPILER "${toolchain_path}/bin/arm-linux-gnueabihf-g++")
```

Then you can call CMake like this:

```bash
cmake -DCONNEXTDDS_DIR=<connext dir> 
      -DCMAKE_TOOLCHAIN_FILE=<toolchain file created above>
      -DCONNEXTDDS_ARCH=<connext architecture>
      -DCMAKE_BUILD_TYPE=Release 
      -DLEVELDB_DIR=<leveldb directory> 
      -DLEVELDB_INCLUDE=<leveldb include directory> 
      ..
```

## Running *Recorder*

First of all, you need to make sure that the `LevelDbWriter` shared library
is in the system's library path.  On Mac OS X, you must use the variable
`RTI_LD_LIBRARY_PATH` instead of the common `DYLD_LIBRARY_PATH`.

This example will work with any DDS Topic discovered on the DDS Domain ID of 
your choice. Run any application publishing data into that domain, for example
*RTI Shapes Demo* or *RTI DDS Ping*.

*RTI Recording Service* expects you to set an environment variable called 
`LEVELDB_WORKING_DIR`. This variable should point to a directory with write
permissions and where all the LevelDB directories and files will be created.

To run the example, you need to run the following command from the `build`
folder (where the storage writer plugin shared library has been created).

```bash
export LEVELDB_WORKING_DIR=<storage location>
cd build
<connext dir>/bin/rtirecordingservice 
        -cfgFile ../leveldb_recorder.xml
        -cfgName LevelDbIntegration 
        -domainIdBase <your chosen domain ID>
```

Of course, the export command has to be translated to your current OS and
shell. After running this command, you should see the following output:

```bash
RTI Recording Service (Recorder) 6.0.1 starting...
RTI Recording Service started
```

The following directories will be created by *Recorder* after creating the
Storage Writer instance:
- `metadata.dat` database directory. This database will contain two key-value 
  entries. They represent the nanoseconds timestamps where the LevelDB Storage 
  Writer was created (started) and destroyed (stopped).
  *Replay* will request these timestamps to know the time span of the database
  and perform its calculations.
- `DCPSPublication.dat` database directory. This database will store the
  publication discovery samples received by *Recorder*. Only a reduced set of
  all the fields in the `DCPSPublication` data type will be stored, just the
  necessary ones for the Storage Reader to identify topics and types being
  discovered.
- For every discovered and matched (allowed by the XML configuration) DDS Topic,
  *Recorder* will create a database whose name will be defined as 
  `<topic-name>@<domain-id>`. For example, if topic `Square` is discovered
  on domain 54 (this is the Domain ID in the XML configuration, not the one
  achieved by offsetting with the `-domainIdBase` command-line parameter), 
  then *Recorder* will create database (directory) `Square@54` and will store
  all Square samples received in that domain in that database.

## Format of the stored data

The format of the data stored by *Recorder* into the LevelDB databases is 
defined in the `LevelDb_RecorderTypes.idl` file. As LevelDB is a key-value
storage system, we need to define the key type. The key type should allow us to:

- Uniquely identify a DDS sample. In DDS, this can be done by using the 
  *original writer virtual GUID* and the *original virtual sequence number* of
  the sample.
- Quickly access the time the sample was received, represented by the sample's
  reception timestamp.
  
This is reflected in type `UserDataKey` that can be found in the IDL file,
defined as:

```idl
struct UserDataKey {
    int64 reception_timestamp;
    octet original_writer_v_guid[GUID_LENGTH];
    SequenceNumber original_v_seq_nr;
};
```

The `UserDataKey` type is used to store keys for both publication data and user
data samples.

The LevelDB library allows for the customization of the key ordering mechanism.
This example implements a key comparator class that extends LevelDB's 
`Comparator` interface. This implementation achieves ordering of samples in a
strictly time-based manner. Resulting databases order their samples in an
ascending, monotinic order based on the reception timestamp. See class
`UserDataKeyComparator` in files `Utils.hpp` and `Utils.cxx`.

### Publication Data

The `DCPSPublication` samples received by *Recorder* will be stored in the
publication database mentioned in the above paragraph. However, for performance
and simplicity reasons, not all the fields in the `DCPSPublication` type will
be stored. The following IDL type describes what information is serialized and
stored in the database:

```idl
struct ReducedDCPSPublication {
    boolean valid_data;
    string<256> topic_name;
    string<256> type_name;
    sequence<octet> type;
};
```

### User Data

The user-data samples received by *Recorder* will be stored in their own LevelDB
database. While we will serialize to CDR and store the whole data sample, there
is no need to store all the Sample Info fields alongside with this data. In fact,
for simplicity and performance reasons, we will only store the valid data flag
along with the user data sample. The following IDL type will be used as the value
(and the aforementioned `UserDataKey` type will be used for the key):

```idl
struct UserDataValue {
    boolean valid_data;
    sequence<char> data_blob;
};
```

## Running *Replay* or *Converter* to access the data

After Recorder has already recorded a database in the working directory provided
by the user, data can be accessed by using the Storage Reader side of this
implementation.

> **Note**:
>
> As mentioned above, both LevelDB and this example are _not prepared_ to have
> *Replay*, *Converter* or any other process access the database while 
> *Recorder* is still running. In general, only one process should access the
> database at a time.
> This is not the case with the built-in SQLite implementation available in
> *RTI Recording Service*, where it is possible to run *Replay* and/or
> *Converter* while *Recorder* is still recording.

To run *Replay* or *Converter*, the `LevelDbReader` shared library should be
accessible to the application, by setting up the environment correctly
(adding the path to the system's library lookup path). On Mac OS X, you must 
use the variable `RTI_LD_LIBRARY_PATH` instead of the common 
`DYLD_LIBRARY_PATH`.

If you recorded one of `Connext DDS`'s demo topics, like Shapes Demo or
RTI DDS Ping, you can run them in subscribing mode now to see the samples
coming from *Replay*. If you're running *Converter*, you can check the
generated SQLite database with the SQLite command-line application or
the SQLite viewer application of your choice.

*RTI Recording Service* expects you to set an environment variable called 
`LEVELDB_WORKING_DIR`. This variable should point to a directory with write
permissions and where all the LevelDB directories and files will be created.

To run *Replay*, you just need to run the following command from the `build`
folder (where the storage reader plugin shared library has been created).

```bash
export LEVELDB_WORKING_DIR=<storage location>
cd build
<connext dir>/bin/rtireplayservice 
        -cfgFile ../leveldb_replay.xml
        -cfgName LevelDbIntegration 
        -domainIdBase <your chosen domain ID>
```

You should see the samples in the file being published and received by the
subscribing application.

The supplied XML *Converter* configuration contains two configurations:
one to convert to SQLite CDR format (`LevelDb_To_SQLiteCDR`), and another 
one to convert to SQLite JSON format (`LevelDb_To_SQLiteJSON`). To run
*Converter*, run the following command:

```bash
export LEVELDB_WORKING_DIR=<storage location>
cd build
<connext dir>/bin/rticonverter 
        -cfgFile ../leveldb_converter.xml
        -cfgName LevelDb_To_SQLiteCDR
```

## Description of the code

This example is divided in three different parts: the writing side plug-in,
the reading side plug-in and a tester application (building it can be 
skipped).

The following sections assume that you have read the section above,
[Format of the stored data](#format-of-the-stored-data)

### The LevelDbWriter class family

The writing side of this example is implemented in class `LevelDbWriter`.
This class will be loaded by *Recorder* or by *Converter* to store data
into a LevelDB database (in the case of *Converter*, the class will be
created when converting from a different format _into_ LevelDB format).

Class **`LevelDbWriter`** extends API class 
`rti::recording::storage::StorageWriter` and acts as a factory to
create `StorageStreamWriter` and `PublicationStorageWriter` objects. It
also defines a method to destroy the instances created by the factory
methods. It will create the `metadata.dat` database and set the start
time to be the time the constructor is called, and the end time to be
the time the destructor is called.

Given the above, there are two other classes defined for writing data:

- **`LevelDbStreamWriter`**: this class extends the API class
  `rti::recording::storage::DynamicDataStorageStreamWriter`. This API
  class is a specialization of class `StorageStreamWriter` set up to
  work with Dynamic Data samples. When a user-data topic is discovered
  and matches the filters in the *Recorder* or *Converter* 
  configuration, the application will ask the `LevelDbWriter` instance
  to create an instance of this class, by calling method
  `create_stream_writer()`.
  The instances of this class will create the different 
  `<topic-name>@<domain-id>` databases where samples of the different 
  topics will be stored. 
  The main method this class has to implement is `store()`. This
  method will be called by the application when samples are available
  to be written to storage. The samples and their associated info
  objects will be passed to the method.
- **`PubDiscoveryLevelDbWriter`**: this class extends the API class
  `rti::recording::storage::PublicationStorageWriter`. This class
  is in charge of storing samples of the built-in `DCPSPublication`
  topic. *Recorder* will ask the `LevelDbWriter` instance to create
  an instance of this class during startup, by calling method 
  `create_publication_writer()`.
  The instance of this class will be in charge of creating the 
  `DCPSPublication.dat` database.
  The main method this class implements is the `store()` method.
  Available samples of topic `DCPSPublication` and their associated
  sample info objects will be passed to the call.
  
#### Implementation details of class `LevelDbStreamWriter`

This Storage Writer plug-in works by storing data in _serialized_
CDR format, by using the types described in the above section
([Format of the stored data](#format-of-the-stored-data)). The code
for the types defined in file `LevelDb_RecorderTypes.idl` will be
generated automatically. The generated code includes methods to
serialize and deserialize samples to and from CDR format.

The main members of the `LevelDbStreamWriter` class are:

- `data_db_`: this is the LevelDB database object (leveldb::DB) that 
  will be used to store the data.
- `key_comparator`: the custom key comparator will be passed when
  the creation of the database happens. The key comparator checks
  the reception timestamp of the samples and returns them in
  monotonic ascending order.
- `key_buffer_`: a byte (char) buffer used to store the serialized
  key sample.
- `value_buffer_`: a byte (char) buffer used to store the serialized
  value sample.

**Constructor**

The constructor of the class has to open the database where the
new topic will be stored. It will construct the name of the 
database with the stream name, taken from the `StreamInfo` object
passed as a parameter, and the domain ID, provided as a parameter
too.

Another key aspect of the constructor is to provide a sensible
initialization of the buffers (byte vectors) that will be used
to serialize the samples stored in the database. This will
minimize the number of memory allocations performed by the
vectors.

For the key buffer's initialization size, we will use the key
type's max sample serialized size:

```cpp
const StructType& key_type = rti::topic::dynamic_type<UserDataKey>::get();
key_buffer_.resize(key_type.cdr_serialized_sample_max_size());
```

In the case of the value buffer, the serialized sample's max size
not only depends on the size of type `UserDataValue`, but mostly
on the size of the topic's type's sample max serialized size. Hence,
we first get the type from the `StreamInfo` object, which is
represented as a native `DDS_TypeCode` pointer object. With it, we
do the following:

- If the type is unbounded, we initialize the buffer to 8 KB.

- Else, we get the max serialized sample from the type and we
  resize the buffer to that size. 
  
This is the code snippet for the above calculations:

```cpp
DDS_TypeCode *type = static_cast<DDS_TypeCode *>(
        stream_info.type_info().type_representation());
DDS_ExceptionCode_t ex = DDS_NO_EXCEPTION_CODE;
if (DDS_TypeCode_is_unbounded(type, DDS_BOOLEAN_FALSE, &ex)
        && ex != DDS_NO_EXCEPTION_CODE) {
    value_buffer_.resize(8192);
} else {
    DDS_UnsignedLong size = DDS_TypeCode_get_cdr_serialized_sample_max_size(
            type,
            DDS_AUTO_DATA_REPRESENTATION,
            &ex);
    if (ex != DDS_NO_EXCEPTION_CODE) {
        // ... handle error
    }
    value_buffer_.resize((std::vector<char>::size_type) size);
}
```

**Store method**

The `store()` method will be called by *Recorder* when samples of the
topic are available to be stored into the database. So the main goal
of this method is:

- To iterate over all samples, passed in as a vector of pointers to
  `dds::core::xtypes::DynamicData`, and all sample info objects.
  
    - For every sample, prepare a **key** that is unique: use the
      *original publication virtual GUID* and the *original publication
      virtual sequence number*. Store the reception timestamp
      alongside, so we can quickly implement time-series ordering
      of the samples (see class `UserDataKeyComparator`). All these
      fields are available in the `SampleInfo` object.
      
    - For every sample, prepare a **value** entry: get the valid data
      flag and store it in the value object. If the sample is valid,
      serialize the sample into CDR. Here is the code snippet:
        ```cpp
        UserDataValue value;
        value.valid_data(sample_info.valid());
        if (sample_info.valid()) {
            const dds::core::xtypes::DynamicData& sample = *(sample_seq[i]);
            rti::core::xtypes::to_cdr_buffer(value.data_blob(), sample);
        }
        ```
      
    - Then serialize the `UserDataValue` sample into CDR.
    
        ```cpp
        dds::topic::topic_type_support<UserDataValue>::to_cdr_buffer(value_buffer_, value);
        ```
        
One important aspect to notice is that we insert all the samples
passed in to the method as one single database operation, by using
the `leveldb::WriteBatch` class. This class is where the key-value
pairs are `Put()` into, to finally call `Write()` to actually
write the samples into storage.
    
#### Implementation details of class `PubDiscoveryLevelDbWriter`

Publication data is also stored in serialized format. Type 
`UserDataKey` is used as the key data, and type 
`ReducedDCPSPublication` is used as the value data. The code
for the types defined in file `LevelDb_RecorderTypes.idl` will be
generated automatically. The generated code includes methods to
serialize and deserialize samples to and from CDR format.

The main members of the `PubDiscoveryLevelDbWriter` class are:

- `discovery_db_`: this is the LevelDB database object (leveldb::DB)
  that will be used to store the `DCPSPublication`.
- `key_comparator`: the custom key comparator will be passed when
  the creation of the database happens. The key comparator checks
  the reception timestamp of the samples and returns them in
  monotonic ascending order.
- `key_buffer_`: a byte (char) buffer used to store the serialized
  key sample.
- `value_buffer_`: a byte (char) buffer used to store the serialized
  value sample.

**Constructor**

The constructor of this class opens the publication database, with
fixed name (`DCPSPublication.dat`).

Here the key value's byte buffer is also initialized properly, using
the type's samples max serialized size:

```cpp
const StructType& key_type = rti::topic::dynamic_type<UserDataKey>::get();
key_buffer_.resize(key_type.cdr_serialized_sample_max_size());
```

However, we do not initialize the data value buffer. The reason 
behind this is that we are going to store the serialized type-object
(coming from discovery) in the stored information. We cannot know in
advance the size of the types that we may discover. So we will let
this buffer adapt the size dynamically as types are discovered.

**Store method**

When *Recorder* discovers a new type it will call the `store()`
method in this class to store the necessary discovery information
about the recorded topics and types. The method receives a vector
of `dds::topic::PublicationBuiltinTopicData` pointers and a
vector of associated `dds::sub::SampleInfo` pointer objects.

The main goal of this method is:

- To iterate over all the publication data pointers and the
  sample info objects:
  
    - For every sample, prepare a **key** that is unique: use the
      *original publication virtual GUID* and the *original publication
      virtual sequence number*. Store the reception timestamp
      alongside, so we can quickly implement time-series ordering
      of the samples (see class `UserDataKeyComparator`). All these
      fields are available in the `SampleInfo` object.
      
    - For every sample, prepare a **value** entry: get the valid data
      flag and store it in the value object. If the sample is valid,
      get the topic name and type name, and set them in the 
      `ReducedDCPSPublication` sample:
      
        ```cpp
        ReducedDCPSPublication reduced_sample;
        reduced_sample.valid_data(sample_info.valid());
        if (sample_info.valid()) {
            reduced_sample.topic_name(sample.topic_name());
            reduced_sample.type_name(sample.type_name());
            // ...

        ```
        
      If the type is available (type information is not always 
      transmitted through discovery), then we need to get the 
      serialized version to be stored.
        ```cpp
        const dds::core::optional<dds::core::xtypes::DynamicType> type =
                sample->type();
        if (type.is_set()) {
            const dds::core::xtypes::DynamicType& dynamic_type = type.get();
            const DDS_TypeCode& native_type = dynamic_type.native();
            DDS_TypeObject *type_object =
                    DDS_TypeObject_create_from_typecode(&native_type);
            if (type_object == nullptr) {
                // handle error condition
                // ...
                continue;
            }
            std::shared_ptr<DDS_TypeObject> type_object_ptr(
                    type_object,
                    DDS_TypeObject_delete);
            uint32_t type_object_buffer_len =
                    DDS_TypeObject_get_serialized_size(type_object);
            std::vector<uint8_t> typeobject_buffer(type_object_buffer_len);
            if (DDS_TypeObject_serialize(
                    type_object,
                    (char *) &(typeobject_buffer[0]),
                    &type_object_buffer_len) != DDS_RETCODE_OK) {
                // handle error condition
                // ...
                continue;
            }
            reduced_sample.type(typeobject_buffer);
        }
        ```
      
    - Then serialize the `ReducedDCPSPublication` sample into CDR.
    
        ```cpp
        dds::topic::topic_type_support<ReducedDCPSPublication>::to_cdr_buffer(value_buffer_, value);
        ```

Like with user-data, we insert all the samples passed in to the 
method as one single database operation, by using the 
`leveldb::WriteBatch` class.

### The LevelDbReader class family

The reading side of this example is implemented in the `LevelDbReader`
classes. These classes are in charge of loading and interpreting
a LevelDB database representing a recording previously
recorded with *Recorder* and the `LevelDbWriter` plug-in. *Replay*
and *Converter* (in the case of *Converter*, the class will be
created when converting _from_ LevelDB format into a different 
format).

Class **`LevelDbReader`** extends API class 
`rti::recording::storage::StorageReader` and acts as a factory to
create `StorageStreamReader` and `StorageStreamInfoReader` objects. It
also defines methods to destroy the instances created by the factory
methods.

Given the above, there are two other classes defined for writing data:

- **`LevelDbStreamInfoReader`**: this class extends the API class
  `rti::recording::storage::StorageStreamInfoReader`. This class
  is in charge of reading serialized samples of type 
  `ReducedDCPSPublication`. *Replay* will ask the `LevelDbReader` 
  instance to create an instance of this class during startup, 
  by calling method `create_stream_info_reader()`.
  The instance of this class will be in charge of reading from the 
  `DCPSPublication.dat` database.
  The main methods this class implements are the `read()` and
  `return_loan()` methods.
  It also implements two methods that *Replay* uses to obtain the
  database's start and end times, `service_start_time()` and
  `service_stop_time()`.
  It also implements the `fisnished()` method to tell *Replay* or
  *Converter* when there are no more discovery samples to read.
  The `reset()` method is used by *Replay* when the looping
  option is enabled.
  
- **`LevelDbStreamReader`**: this class extends the API class
  `rti::recording::storage::DynamicDataStorageStreamReader`. This API
  class is a specialization of class `StorageStreamReader` set up to
  work with Dynamic Data samples. When a user-data topic is discovered
  (read by Replay using the stream info reader above) and matches the
  filters in the *Replay* or *Converter* configuration, the application 
  will ask the `LevelDbReader` instance to create an instance of this 
  class, by calling method `create_stream_reader()`.
  The instances of this class will look for and read the appropriate 
  `<topic-name>@<domain-id>` database where samples of the topic are 
  stored. 
  The main method this class has to implement are `read()` and 
  `return_loan()`. This methods will be called by the application when 
  it's time to read new samples from the database. The `return_loan()`
  operation is called by the application after the read samples have
  been used so that resources associated with the samples can be freed. 
  It also implements the `finished()` method to tell *Replay* or
  *Converter* when there are no more samples for the topic to read.
  The `reset()` method is used by *Replay* when the looping option 
  is enabled.
  
#### Implementation details of class `LevelDbStreamReader`

This Storage Reader plug-in works by reading data stored in 
_serialized_ CDR format, by using the types described in the above section
([Format of the stored data](#format-of-the-stored-data)). The code
for the types defined in file `LevelDb_RecorderTypes.idl` will be
generated automatically. The generated code includes methods to
serialize and deserialize samples to and from CDR format.

The main members of the `LevelDbStreamReader` class are:

- `data_db_`: this is the LevelDB database object (`leveldb::DB`) that 
  will be used to read the data and that was recorded by *Recorder* using
  the Storage Writer implementation in this example.
- `db_iterator_`: the class keeps a database iterator (`leveldb::Iterator`)
  that points to the next sample to be read.
- `current_timestamp_`: the current sample's reception timestamp.
- `key_comparator`: the custom key comparator will be passed when
  the creation of the database happens. The key comparator checks
  the reception timestamp of the samples and returns them in
  monotonic ascending order.
- `key_buffer_`: a byte (char) buffer used to store the serialized
  key sample, read from the database.
- `value_buffer_`: a byte (char) buffer used to store the serialized
  value sample.
- `loaned_samples_`: every time the `read()` method is called, the
  samples read are stored here. This vector of samples will be freed
  when the `return_loan()` operation is called.
- `loaned_infos_`: every time the `read()` method is called, the
  sample info objects related to the read samples are stored here.
  This vector will be freed when the `return_loan()` operation is
  called.

**Constructor**

The constructor of the class attempts to open the expected database,
given the topic name (`StreamInfo` object's stream name) and the
domain ID.
  
Here the key value's byte buffer is also initialized properly, using
the type's samples max serialized size:

```cpp
const StructType& key_type = rti::topic::dynamic_type<UserDataKey>::get();
key_buffer_.resize(key_type.cdr_serialized_sample_max_size());
```

The constructor also has to prepare the database iterator. Given
the start timestamp and end timestamp passed as parameters, the
iterator has to be set to the first sample that lies within that
time range:

```cpp
db_iterator_ = std::unique_ptr<leveldb::Iterator>(
        data_db_->NewIterator(leveldb::ReadOptions()));
for (db_iterator_->SeekToFirst(); db_iterator_->Valid(); ) {
    UserDataKey current_key = slice_to_user_type<UserDataKey>(
            db_iterator_->key(),
            key_buffer_);
    int64_t current_timestamp = current_key.reception_timestamp();
    if (current_timestamp_ >= start_timestamp_
            && current_timestamp_ <= end_timestamp_) {
        break;
    } else {
        db_iterator_->Next();
    }
}
```

The constructor also pre-allocates the necessary size in the
value buffer, using the same logic as with the stream writers
above:

- If the type is unbounded, we initialize the buffer to 8 KB.

- Else, we get the max serialized sample from the type and we
  resize the buffer to that size. 
  
This is the code snippet for the above calculations:

```cpp
DDS_TypeCode *type = static_cast<DDS_TypeCode *>(
        stream_info.type_info().type_representation());
DDS_ExceptionCode_t ex = DDS_NO_EXCEPTION_CODE;
if (DDS_TypeCode_is_unbounded(type, DDS_BOOLEAN_FALSE, &ex)
        && ex != DDS_NO_EXCEPTION_CODE) {
    value_buffer_.resize(8192);
} else {
    DDS_UnsignedLong size = DDS_TypeCode_get_cdr_serialized_sample_max_size(
            type,
            DDS_AUTO_DATA_REPRESENTATION,
            &ex);
    if (ex != DDS_NO_EXCEPTION_CODE) {
        // ... handle error
    }
    value_buffer_.resize((std::vector<char>::size_type) size);
}
```

**Read and return loan methods**

The `read()` method receives two empty vectors: one to store the
samples read, with pointers to `dds::core::xtypes::DynamicData` and
another one to store sample info pointers (`dds::sub::SampleInfo`).
It also receives a parameter of type 
`rti::recording::storage::SelectorState`, which provides the
conditions the returned samples have to comply with. In the case
of *Replay*, a time range will be provided. The maximum number
of samples to be returned can be specified too (*Converter* will
use this to limit the number of samples to ask for, given that
*Converter* will impose no time restrictions; *Replay* can also
use this setting).

Given the above, the `read()` method will run a while loop checking
the current sample (the one pointed to by the iterator):

```cpp
UserDataKey current_key;
const int64_t timestamp_limit =
        to_nanosec_timestamp(selector.time_range_end());
const int32_t max_samples = selector.max_samples();
int32_t num_samples_read = 0;
bool next_sample_valid = db_iterator_->Valid();
while (next_sample_valid
        && current_timestamp <= timestamp_limit
        && (max_samples < 0 ? true : (num_samples_read < max_samples))) {
    // ...
}
```

Inside the loop, it means the current sample pointed to by the
iterator is valid and should be included in the read samples collection.
The following code extracts the sample and deserializes the dynamic
data object if the sample is valid:

```cpp
UserDataValue current_value = slice_to_user_type<UserDataValue>(
        db_iterator_->value(),
        value_buffer_);
std::shared_ptr<DynamicData> sample(
        std::make_shared<DynamicData>(type_));
if (current_value.valid_data()) {
    rti::core::xtypes::from_cdr_buffer(
            *(sample.get()),
            current_value.data_blob());
}
```

Notice the use of function `slice_to_user_type()`, that transforms
a LevelDB Slice into the desired type (by deserializing it).

The following code snippet is the advancement of the database
iterator to the next sample:

```cpp
db_iterator_->Next();
if (db_iterator_->Valid()) {
    current_key = slice_to_user_type<UserDataKey>(
            db_iterator_->key(),
            key_buffer_);
    current_timestamp_ = current_key.reception_timestamp();
    if (current_timestamp_ > end_timestamp_) {
        next_sample_valid = false;
    } else {
        next_sample_valid = true;
    }
} else {
    next_sample_valid = false;
}
```

Notice that we need to check if the next sample, if any,
is within the stream rea'ders time range (`current_timestamp`
has less than or equal to the `end_timestamp_`).

The `return_loan()` method clears the vectors used to 
return the samples and sample infos, but also the internal
`loaned_samples_` and `loaned_infos_` vectors.

It also sets the `finished_` flag when the iterator has
finished iterating, or else when the next sample's timestamp
is out of time range.

#### Implementation details of class `LevelDbStreamInfoReader`

When *Recorder* runs, publication data is stored in serialized 
format. Type `UserDataKey` is used as the key data, and type 
`ReducedDCPSPublication` is used as the value data. The code
for the types defined in file `LevelDb_RecorderTypes.idl` will be
generated automatically. The generated code includes methods to
serialize and deserialize samples to and from CDR format.

The main members of the `LevelDbStreamInfoReader` class are:

- `metadata_db_`: this is the LevelDB database object (`leveldb::DB`)
  that was used to store the start and end timestamps of the
  service run.
- `discovery_db_`: this is the LevelDB database object (`leveldb::DB`)
  where the DCPSPublication information was stored. The expected 
  database name is `DCPSPublication.dat`.
- `db_iterator_`: the class keeps a database iterator (`leveldb::Iterator`)
  that points to the next sample to be read in the discovery database.
- `current_timestamp_`: the current sample's reception timestamp.
- `key_comparator`: the custom key comparator will be passed when
  the creation of the database happens. The key comparator checks
  the reception timestamp of the samples and returns them in
  monotonic ascending order.
- `key_buffer_`: a byte (char) buffer used to store the serialized
  key sample.
- `value_buffer_`: a byte (char) buffer used to store the serialized
  value sample.
- `loaned_stream_infos_`: every time the `read()` method is called, the
  samples read are stored here. This vector of samples will be freed
  when the `return_loan()` operation is called.

**Constructor**

The constructor of this class opens the publication database, with
fixed name (`DCPSPublication.dat`). Similarly to the Stream Readers 
above, the Stream Info Reader also pre-allocates the `key_buffer_`
vector. Setting up the iterator works similarly too: we have to find
the first valid sample within the specified time range.

The constructor of this class also opens the `metadata.dat` LevelDB
database where *Recorder* stored the start and stop times of the
database values, and reads those values. These values are used in methods
`service_start_time()` and `service_stop_time()`. *Replay* needs these
values to establish a reference time range.

**Read and return loan methods**

The `read()` method is passed a vector of pointers to `rti::routing::StreamInfo`
objects. This is where the read `DCPSPublication` samples should be stored and
returned. 

It also receives a parameter of type 
`rti::recording::storage::SelectorState`, which provides the
conditions the returned samples have to comply with. These conditions include
time range information (usually an end timestamp) and the maximum number of
samples to be read.

Given the above, the `read()` method will run a while loop checking
the current sample (the one pointed to by the iterator):

```cpp
UserDataKey current_key;
const int64_t timestamp_limit =
        to_nanosec_timestamp(selector.time_range_end());
const int32_t max_samples = selector.max_samples();
int32_t num_samples_read = 0;
bool next_sample_valid = db_iterator_->Valid();
while (next_sample_valid
        && current_timestamp <= timestamp_limit
        && (max_samples < 0 ? true : (num_samples_read < max_samples))) {
    // ...
}
```

Inside the loop, it means the current sample pointed to by the
iterator is valid and should be included in the stream infos collection.
The following code deserializes the `ReducedDCPSPublication` sample, and
if the sample is valid, then it deserializes the stored serialized type-object.
Take into account, that the `StreamInfo` object expects the type representation
to be a _type-code_, so we also convert the deserialized type-object into a
type-code:

```cpp
ReducedDCPSPublication reduced_sample =
        slice_to_user_type<ReducedDCPSPublication>(
                db_iterator_->value(),
                value_buffer_);
std::shared_ptr<rti::routing::StreamInfo> stream_info =
        std::make_shared<rti::routing::StreamInfo>("", "");
if (reduced_sample.valid_data()) {
    stream_info->stream_name(reduced_sample.topic_name());
    stream_info->type_info().type_name(reduced_sample.type_name());
    stream_info->type_info().type_representation_kind(
            rti::routing::TypeRepresentationKind::DYNAMIC_TYPE);
    DDS_TypeObject *type_obj =
            DDS_TypeObjectFactory_create_typeobject_from_serialize_buffer(
                    type_factory.get(),
                    (const char *) reduced_sample.type().data(),
                    reduced_sample.type().size());
    if (type_obj == nullptr) {
        // handle error condition
        continue;
    }
    std::shared_ptr<DDS_TypeObject> type_ptr(
            type_obj,
            TypeObjectDeleter(type_factory.get()));
    DDS_TypeCode *type_code = DDS_TypeObject_convert_to_typecode(type_obj);
    if (type_code == nullptr) {
        // handle error condition
        continue;
    }
    stream_info->type_info().type_representation(type_code);
}
```
The following snippet shows how advancing to the next sample is done. The
iterator is advanced and we check if the end has been reached (whether the
next value is valid or not). If it's valid, we also verify if the newly
pointed-to sample's reception timestamp is within the stream's time range
(verify it's less or equal than the `end_timestamp_`):

```cpp
db_iterator_->Next();
if (db_iterator_->Valid()) {
    current_key = slice_to_user_type<UserDataKey>(
            db_iterator_->key(),
            key_buffer_);
    current_timestamp_ = current_key.reception_timestamp();
    if (current_timestamp_ > end_timestamp_) {
        next_sample_valid = false;
    } else {
        next_sample_valid = true;
    }
} else {
    next_sample_valid = false;
}
```

The mission of the `return_loan()` method is to free up any resources created
by the `read()` method, and update the `finished_` flag. 

In this case, it's very important that we delete the type-code objects created
during the reading process:

```cpp
for (size_t i = 0; i < sample_seq.size(); i++) {
    if (sample_seq[i]->type_info().type_representation() != nullptr) {
        DDS_TypeCode *type_code = (DDS_TypeCode *)
                sample_seq[i]->type_info().type_representation();
        RTICdrTypeCode_destroyTypeCode((RTICdrTypeCode *) type_code);
    }
}
```
